{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "from nostril import nonsense_detector as nd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "def get_data():\n",
    "  engine_name = f\"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n",
    "  engine = create_engine(engine_name)\n",
    "\n",
    "  query_sql = \"SELECT * FROM fb_data\"\n",
    "  df = pd.read_sql_query(query_sql, con=engine)\n",
    "  return df\n",
    "\n",
    "def process_text(text):\n",
    "  TAG_RE = re.compile(r'<[^>]+>')\n",
    "  text = TAG_RE.sub('',text)\n",
    "  text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "  text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "  return simple_preprocess(text, deacc=True)\n",
    "\n",
    "def map_label_to_class(label):\n",
    "  SOURCES = {\n",
    "    0: [\"nytimes\",\"cnn\",\"nbc\"],\n",
    "    1: [\"FoxNews\",\"DailyMail\",\"NYPost\"],\n",
    "    2: [\"bbcnews\",\"Reuters\",\"APNews\"]\n",
    "  }\n",
    "\n",
    "  for class_label, sources in SOURCES.items():\n",
    "      if label in sources:\n",
    "          return class_label\n",
    "  raise ValueError(f\"Label '{label}' not found in any source categories.\")\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "  df['text'] = df['text'].apply(process_text)\n",
    "  df['source'] = df['source'].apply(map_label_to_class)\n",
    "  df.drop(columns=['id'])\n",
    "\n",
    "  label_counts = df['source'].value_counts().sort_index()\n",
    "  plt.bar(label_counts.index, label_counts.values, color=['blue', 'red', 'green'])\n",
    "  plt.xticks(label_counts.index, ['left', 'right', 'center'])  \n",
    "  plt.xlabel('Label')\n",
    "  plt.ylabel('Count')\n",
    "  plt.title('Count of Labels')\n",
    "  plt.show()\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc 2 vec\n",
    "def create_d2v(df):\n",
    "  print('Training doc2vec model')\n",
    "  tagged =  [TaggedDocument(words=text, tags=[str(i)]) for i, text in enumerate(df['text'])]\n",
    "  d2v_model = Doc2Vec(dm=1,vector_size=300,min_count=0,workers=4,epochs=40)\n",
    "  d2v_model.build_vocab(tagged)\n",
    "  d2v_model.train(utils.shuffle(tagged),total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
    "  print('Finished Training!!!')\n",
    "  return d2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset initialization\n",
    "class FBData(Dataset):\n",
    "  def __init__(self,df,d2v_model):\n",
    "    df.drop(columns=['id'])\n",
    "    \n",
    "    self.data = df\n",
    "    self.d2v = d2v_model\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    text = self.data['text'][idx]\n",
    "    label = self.data['source'][idx]\n",
    "    vectorized_text = self.d2v.infer_vector(text)\n",
    "    return torch.tensor(vectorized_text),torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class BiasClassificationModel(nn.Module):\n",
    "  def __init__(self,input_size,hidden_dim,num_class):\n",
    "    super(BiasClassificationModel,self).__init__()\n",
    "    self.fc1 = nn.Linear(input_size,hidden_dim)\n",
    "    self.fc2 = nn.Linear(hidden_dim,num_class)\n",
    "    self.softmax = nn.Softmax(dim=0)\n",
    "  \n",
    "  def forward(self,text):\n",
    "    output = self.fc1(text)\n",
    "    output = self.fc2(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data \n",
    "def split_data(dataset,batch_size):\n",
    "  train_idx, test_idx = train_test_split(range(len(dataset)),test_size=0.2,random_state=42,shuffle=True)\n",
    "  train_dataset = Subset(dataset,train_idx)\n",
    "  test_dataset = Subset(dataset,test_idx)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=collate_batch)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True,collate_fn=collate_batch)\n",
    "  return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test\n",
    "def train(dataloader,model,optimizer,criterion,verbose):\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "  train_acc = 0\n",
    "  total_samples = 0\n",
    "\n",
    "  for text,label in tqdm(dataloader, disable=not verbose):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(text)\n",
    "    loss = criterion(output,label)\n",
    "\n",
    "    train_loss += loss.item()\n",
    "    train_acc += (output.argmax(1) == label).sum().item()\n",
    "    total_samples += text.size(0)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  train_loss/=total_samples\n",
    "  train_acc/=total_samples\n",
    "\n",
    "  return train_loss, train_acc, model, optimizer, criterion\n",
    "\n",
    "def test(dataloader,model,criterion, verbose):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  test_acc = 0\n",
    "  total_samples = 0\n",
    "  test_preds = []\n",
    "  test_labels = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for text,label in tqdm(dataloader,disable=not verbose):\n",
    "      output = model(text)\n",
    "      loss = criterion(output,label)\n",
    "\n",
    "      test_loss += loss.item()\n",
    "      test_acc += (output.argmax(1) == label).sum().item()\n",
    "      total_samples += text.size(0)\n",
    "\n",
    "      test_preds.extend(output.argmax(1).cpu().numpy())\n",
    "      test_labels.extend(label.cpu().numpy())\n",
    "      \n",
    "  test_loss/=total_samples\n",
    "  test_acc/=total_samples\n",
    "  return test_loss,test_acc, test_preds, test_labels\n",
    "\n",
    "def train_test_loop(train_loader,test_loader,num_epochs,model,optimizer,criterion,verbose):\n",
    "  print('Training Starting')\n",
    "\n",
    "  train_losses = []\n",
    "  train_accs = []\n",
    "  test_losses = []\n",
    "  test_accs = []\n",
    "  for epoch in range(num_epochs):\n",
    "    train_loss, train_acc, model, optimizer, criterion = train(train_loader,model,optimizer,criterion,verbose)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    if verbose: print(f'Training Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}')\n",
    "\n",
    "    test_loss, test_acc, test_preds, test_labels  = test(test_loader,model,criterion,verbose)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    if verbose: print(f'Test Epoch [{epoch + 1}/{num_epochs}], Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}')\n",
    "  \n",
    "  print('Training Done, Final Testing Results Are As Shown:')\n",
    "\n",
    "  print(f'Training Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}')\n",
    "  print(f'Test Epoch [{epoch + 1}/{num_epochs}], Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}')\n",
    "\n",
    "  plt.plot(train_losses,label='Train Loss')\n",
    "  plt.plot(test_losses,label='Test Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  plt.plot(train_accs,label='Train Acc')\n",
    "  plt.plot(test_accs,label='Test Acc')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Accuracy %')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  cm = confusion_matrix(test_preds, test_labels)\n",
    "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2])\n",
    "  disp.plot(cmap='Blues')\n",
    "  plt.title('Confusion Matrix: Predicted vs Actual Labels')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text,model,d2v_model):\n",
    "  tokenized_text = process_text(text)\n",
    "  vectorized_text = d2v_model.infer_vector(tokenized_text)\n",
    "  output = model(vectorized_text)\n",
    "  prediction = output.argmax(1).item()\n",
    "\n",
    "  if prediction == 0:\n",
    "    prediction = 'left'\n",
    "  elif prediction == 1:\n",
    "    prediction = 'right'\n",
    "  elif prediction == 2:\n",
    "    prediction = 'center'\n",
    "  else:\n",
    "    raise ValueError(f\"Prediction error.\")\n",
    "\n",
    "  print(f'The text provided leans towards the {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training doc2vec model\n",
      "Finished Training!!!\n",
      "Training Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:03<00:00, 246.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/100], Loss: 1.1068, Accuracy: 0.5334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:00<00:00, 351.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [1/100], Loss: 1.0269, Accuracy: 0.5841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:03<00:00, 250.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch [2/100], Loss: 0.8992, Accuracy: 0.6202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:00<00:00, 335.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [2/100], Loss: 1.0638, Accuracy: 0.5467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:03<00:00, 238.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch [3/100], Loss: 0.9220, Accuracy: 0.6143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:00<00:00, 338.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [3/100], Loss: 0.8704, Accuracy: 0.5981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:03<00:00, 248.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch [4/100], Loss: 0.9880, Accuracy: 0.6213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:00<00:00, 357.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [4/100], Loss: 0.8136, Accuracy: 0.6542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:03<00:00, 246.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch [5/100], Loss: 0.8819, Accuracy: 0.6331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:00<00:00, 353.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [5/100], Loss: 0.8850, Accuracy: 0.6121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:03<00:00, 237.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch [6/100], Loss: 0.9062, Accuracy: 0.6284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:00<00:00, 351.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [6/100], Loss: 0.8858, Accuracy: 0.5981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:03<00:00, 240.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch [7/100], Loss: 0.9483, Accuracy: 0.6237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:00<00:00, 345.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [7/100], Loss: 1.0593, Accuracy: 0.5935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:03<00:00, 250.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch [8/100], Loss: 0.9443, Accuracy: 0.6272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:00<00:00, 347.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [8/100], Loss: 0.9853, Accuracy: 0.5654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:03<00:00, 239.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch [9/100], Loss: 0.8645, Accuracy: 0.6483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:00<00:00, 356.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [9/100], Loss: 0.8985, Accuracy: 0.6589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:03<00:00, 241.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch [10/100], Loss: 0.8808, Accuracy: 0.6284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:00<00:00, 348.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch [10/100], Loss: 0.8869, Accuracy: 0.6355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:03<00:00, 230.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch [11/100], Loss: 0.8931, Accuracy: 0.6471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 123/214 [00:00<00:00, 350.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     predict_text(text,dataset,model,d2v_model)\n\u001b[0;32m     25\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBreaking News: Hersh Goldberg-Polin, an American-Israeli who had been held in Gaza by Hamas for nearly 11 months, died in captivity, President Biden announced.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[107], line 20\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(text, batch_size, num_epochs, verbose)\u001b[0m\n\u001b[0;32m     17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[0;32m     18\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrain_test_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     23\u001b[0m   predict_text(text,dataset,model,d2v_model)\n",
      "Cell \u001b[1;32mIn[105], line 63\u001b[0m, in \u001b[0;36mtrain_test_loop\u001b[1;34m(train_loader, test_loader, num_epochs, model, optimizer, criterion, verbose)\u001b[0m\n\u001b[0;32m     60\u001b[0m train_accs\u001b[38;5;241m.\u001b[39mappend(train_acc)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Epoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m test_loss, test_acc, test_preds, test_labels  \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m test_losses\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[0;32m     65\u001b[0m test_accs\u001b[38;5;241m.\u001b[39mappend(test_acc)\n",
      "Cell \u001b[1;32mIn[105], line 35\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(dataloader, model, criterion, verbose)\u001b[0m\n\u001b[0;32m     32\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 35\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\.Work\\projects\\Media-Bias-Detector\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\.Work\\projects\\Media-Bias-Detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\.Work\\projects\\Media-Bias-Detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\.Work\\projects\\Media-Bias-Detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\.Work\\projects\\Media-Bias-Detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\.Work\\projects\\Media-Bias-Detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[102], line 15\u001b[0m, in \u001b[0;36mFBData.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     13\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][idx]\n\u001b[0;32m     14\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m][idx]\n\u001b[1;32m---> 15\u001b[0m vectorized_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(vectorized_text),torch\u001b[38;5;241m.\u001b[39mtensor(label)\n",
      "File \u001b[1;32mc:\\.Work\\projects\\Media-Bias-Detector\\.venv\\Lib\\site-packages\\gensim\\models\\doc2vec.py:651\u001b[0m, in \u001b[0;36mDoc2Vec.infer_vector\u001b[1;34m(self, doc_words, alpha, min_alpha, epochs)\u001b[0m\n\u001b[0;32m    646\u001b[0m         train_document_dm_concat(\n\u001b[0;32m    647\u001b[0m             \u001b[38;5;28mself\u001b[39m, doc_words, doctag_indexes, alpha, work, neu1,\n\u001b[0;32m    648\u001b[0m             learn_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, learn_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, doctag_vectors\u001b[38;5;241m=\u001b[39mdoctag_vectors, doctags_lockf\u001b[38;5;241m=\u001b[39mdoctags_lockf\n\u001b[0;32m    649\u001b[0m         )\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 651\u001b[0m         \u001b[43mtrain_document_dm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoctag_indexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneu1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlearn_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoctag_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoctag_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoctags_lockf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoctags_lockf\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m     alpha \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m alpha_delta\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doctag_vectors[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(text,batch_size,num_epochs,verbose):\n",
    "\n",
    "  if nd.nonsense(text):\n",
    "    raise Exception(\"Gibberish Text Detected\") \n",
    "  \n",
    "  df = get_data()\n",
    "  df = process_data(df)\n",
    "  d2v_model = create_d2v(df)\n",
    "  dataset = FBData(df,d2v_model)\n",
    "  train_loader, test_loader = split_data(dataset,batch_size)\n",
    "\n",
    "  input_size = d2v_model.vector_size\n",
    "  hidden_dim = 64\n",
    "  num_classes = 3\n",
    "\n",
    "  model = BiasClassificationModel(input_size,hidden_dim,num_classes)\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "  criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "  train_test_loop(train_loader,test_loader,num_epochs,model,optimizer,criterion,verbose)\n",
    "\n",
    "  if(text is not None):\n",
    "    predict_text(text,dataset,model,d2v_model)\n",
    "\n",
    "text = \"Breaking News: Hersh Goldberg-Polin, an American-Israeli who had been held in Gaza by Hamas for nearly 11 months, died in captivity, President Biden announced.\"\n",
    "main(text,batch_size=1,num_epochs=100,verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
